{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/williamervin7/titanic-survival-prediction?scriptVersionId=240851121\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"This notebook handles feature preparation, model training, and evaluation for the Titanic dataset.","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:57:51.886307Z","iopub.execute_input":"2025-05-20T15:57:51.886579Z","iopub.status.idle":"2025-05-20T15:57:52.261457Z","shell.execute_reply.started":"2025-05-20T15:57:51.886559Z","shell.execute_reply":"2025-05-20T15:57:52.26017Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/titanic/train.csv\n/kaggle/input/titanic/test.csv\n/kaggle/input/titanic/gender_submission.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"#read in data\ntrain = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntrain.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:57:52.263183Z","iopub.execute_input":"2025-05-20T15:57:52.263688Z","iopub.status.idle":"2025-05-20T15:57:52.329164Z","shell.execute_reply.started":"2025-05-20T15:57:52.263659Z","shell.execute_reply":"2025-05-20T15:57:52.328096Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n\n                                                Name     Sex   Age  SibSp  \\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                             Heikkinen, Miss. Laina  female  26.0      0   \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n4                           Allen, Mr. William Henry    male  35.0      0   \n\n   Parch            Ticket     Fare Cabin Embarked  \n0      0         A/5 21171   7.2500   NaN        S  \n1      0          PC 17599  71.2833   C85        C  \n2      0  STON/O2. 3101282   7.9250   NaN        S  \n3      0            113803  53.1000  C123        S  \n4      0            373450   8.0500   NaN        S  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>Heikkinen, Miss. Laina</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Allen, Mr. William Henry</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"### Check for the missing values in the data set","metadata":{}},{"cell_type":"code","source":"df = train.copy() # keep original intact\ndf.isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:57:52.330303Z","iopub.execute_input":"2025-05-20T15:57:52.33056Z","iopub.status.idle":"2025-05-20T15:57:52.340616Z","shell.execute_reply.started":"2025-05-20T15:57:52.330538Z","shell.execute_reply":"2025-05-20T15:57:52.339636Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"PassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"# Feature engerning","metadata":{}},{"cell_type":"markdown","source":"We grouped by Pclass and Sex because age isn’t missing at random—passenger class and gender give useful clues about how old someone probably was.","metadata":{}},{"cell_type":"code","source":"age_medians = df.groupby(['Pclass','Sex'])['Age'].median()\nprint(\"Empty median cells:\", age_medians.isna().sum())   # should be 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:57:52.342994Z","iopub.execute_input":"2025-05-20T15:57:52.343319Z","iopub.status.idle":"2025-05-20T15:57:52.377453Z","shell.execute_reply.started":"2025-05-20T15:57:52.343294Z","shell.execute_reply":"2025-05-20T15:57:52.37653Z"}},"outputs":[{"name":"stdout","text":"Empty median cells: 0\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"Define a function that fills in missing Age values by looking up the median Age for passengers with the same Pclass and Sex.\n","metadata":{}},{"cell_type":"code","source":"# Define a function that looks up the right median\ndef age_imputer(row):\n    if pd.isna(row['Age']):\n        return age_medians.loc[(row['Pclass'], row['Sex'])]\n    else:\n        return row['Age']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:57:52.378475Z","iopub.execute_input":"2025-05-20T15:57:52.378791Z","iopub.status.idle":"2025-05-20T15:57:52.447188Z","shell.execute_reply.started":"2025-05-20T15:57:52.378763Z","shell.execute_reply":"2025-05-20T15:57:52.446127Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"Apply the age_imputer function","metadata":{}},{"cell_type":"code","source":"df['Age'] = df.apply(age_imputer, axis = 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:57:52.448517Z","iopub.execute_input":"2025-05-20T15:57:52.448871Z","iopub.status.idle":"2025-05-20T15:57:52.495391Z","shell.execute_reply.started":"2025-05-20T15:57:52.448838Z","shell.execute_reply":"2025-05-20T15:57:52.494178Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"print(\"NaNs remaining :\", df['Age'].isna().sum()) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:57:52.496369Z","iopub.execute_input":"2025-05-20T15:57:52.496711Z","iopub.status.idle":"2025-05-20T15:57:52.503731Z","shell.execute_reply.started":"2025-05-20T15:57:52.496676Z","shell.execute_reply":"2025-05-20T15:57:52.502299Z"}},"outputs":[{"name":"stdout","text":"NaNs remaining : 0\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"Embarked col also has NAN values here we fill with mode","metadata":{}},{"cell_type":"code","source":"# Fill Embarked with mode.\nembarked_mode = df['Embarked'].mode()[0] # grab the single value\ndf['Embarked'] = df['Embarked'].fillna(embarked_mode) # assign back\nprint(\"NaNs remaining :\", df['Embarked'].isna().sum()) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:57:52.505307Z","iopub.execute_input":"2025-05-20T15:57:52.505573Z","iopub.status.idle":"2025-05-20T15:57:52.52827Z","shell.execute_reply.started":"2025-05-20T15:57:52.505551Z","shell.execute_reply":"2025-05-20T15:57:52.527174Z"}},"outputs":[{"name":"stdout","text":"NaNs remaining : 0\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"Set the target variable y using the Survived column. Create a new feature called FamilySize by combining SibSp and Parch, then adding 1 for the passenger. Finally, calculate the median Fare from the training data to use for imputing missing values later.","metadata":{}},{"cell_type":"code","source":"y = df['Survived']\ndf['FamilySize'] = df['SibSp'] + df['Parch'] + 1\nfare_median = df['Fare'].median()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:57:52.529068Z","iopub.execute_input":"2025-05-20T15:57:52.529362Z","iopub.status.idle":"2025-05-20T15:57:52.549619Z","shell.execute_reply.started":"2025-05-20T15:57:52.529341Z","shell.execute_reply":"2025-05-20T15:57:52.54878Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"Extract Title from Name\nThis breaks out groups like Mr, Miss, Master, Dr, etc., which are very predictive","metadata":{}},{"cell_type":"code","source":"df['Title'] = df['Name'].str.extract(r',\\s*(\\w+)\\.').iloc[:,0]\n# Optionally group rare titles\ndf['Title'] = df['Title'].replace(['Mlle', 'Ms'], 'Miss')\ndf['Title'] = df['Title'].replace(['Mme', 'Lady', 'Countess'], 'Mrs')\ndf['Title'] = df['Title'].replace(['Dr', 'Rev', 'Col', 'Major', 'Capt', 'Don', 'Sir', 'Jonkheer'], 'Rare')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:57:52.552801Z","iopub.execute_input":"2025-05-20T15:57:52.553049Z","iopub.status.idle":"2025-05-20T15:57:52.588378Z","shell.execute_reply.started":"2025-05-20T15:57:52.553031Z","shell.execute_reply":"2025-05-20T15:57:52.587318Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"Add FarePerPerson\nThis adjusts for group ticket purchases","metadata":{}},{"cell_type":"code","source":"df['FarePerPerson'] = df['Fare'] / df['FamilySize']\ndf['FarePerPerson'] = df['FarePerPerson'].replace([np.inf, -np.inf], 0)\ndf['FarePerPerson'] = df['FarePerPerson'].fillna(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:57:52.589398Z","iopub.execute_input":"2025-05-20T15:57:52.590156Z","iopub.status.idle":"2025-05-20T15:57:52.611301Z","shell.execute_reply.started":"2025-05-20T15:57:52.590117Z","shell.execute_reply":"2025-05-20T15:57:52.610108Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"Extract Deck from Cabin","metadata":{}},{"cell_type":"code","source":"df['Deck'] = df['Cabin'].str[0].fillna('U')  # U for Unknown","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:57:52.612361Z","iopub.execute_input":"2025-05-20T15:57:52.612681Z","iopub.status.idle":"2025-05-20T15:57:52.635677Z","shell.execute_reply.started":"2025-05-20T15:57:52.612652Z","shell.execute_reply":"2025-05-20T15:57:52.634631Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"### One‑hot‑encode the categorical columns\nSelect the numeric features we want to use in the model, then encode the categorical variables using one-hot encoding (dropping the first category to avoid multicollinearity). Finally, combine the numeric and encoded categorical features into a single DataFrame X for training.","metadata":{}},{"cell_type":"code","source":"num_cols = df[['Age','Fare','FamilySize','FarePerPerson']]\ncat_cols = df[['Sex','Embarked','Pclass','Title', 'Deck']]\n    \ncat_dummies = pd.get_dummies(\n    cat_cols,\n    drop_first=True,\n    dtype='int8'\n)\nX = pd.concat([num_cols, cat_dummies], axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:57:52.637048Z","iopub.execute_input":"2025-05-20T15:57:52.63738Z","iopub.status.idle":"2025-05-20T15:57:52.665941Z","shell.execute_reply.started":"2025-05-20T15:57:52.637343Z","shell.execute_reply":"2025-05-20T15:57:52.664856Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"### Train/test split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.2, random_state=0, stratify=y)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:57:52.666877Z","iopub.execute_input":"2025-05-20T15:57:52.66715Z","iopub.status.idle":"2025-05-20T15:57:53.49928Z","shell.execute_reply.started":"2025-05-20T15:57:52.667127Z","shell.execute_reply":"2025-05-20T15:57:53.498311Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"Import several classification models and set them up with reasonable parameters. Store them in a dictionary to easily loop through. Use 5-fold cross-validation to evaluate each model on the training data, scoring by accuracy. Print the average score for each model to compare performance.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nmodels = {\n    'RandomForest': RandomForestClassifier(n_estimators=300, random_state=0),\n    'GradientBoost': GradientBoostingClassifier(random_state=0),\n    'XGBoost': XGBClassifier(n_estimators=400, learning_rate=0.05,\n                             max_depth=4, random_state=0, use_label_encoder=False, eval_metric='logloss'),\n    'LogisticRegression': LogisticRegression(max_iter=1000),\n    'LGBMClassifier' : LGBMClassifier( n_estimators=400, learning_rate=0.05,\n                             max_depth=4, random_state=0, verbose=-1 )\n}\n\nfor name, clf in models.items():\n    score = cross_val_score(clf, X, y, cv=5, scoring='accuracy').mean()\n    print(f'{name}: {score:.4f}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:57:53.500176Z","iopub.execute_input":"2025-05-20T15:57:53.500687Z","iopub.status.idle":"2025-05-20T15:58:20.666659Z","shell.execute_reply.started":"2025-05-20T15:57:53.500547Z","shell.execute_reply":"2025-05-20T15:58:20.665462Z"}},"outputs":[{"name":"stdout","text":"RandomForest: 0.8069\nGradientBoost: 0.8283\nXGBoost: 0.8339\nLogisticRegression: 0.8261\nLGBMClassifier: 0.8283\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"Use GridSearchCV to find the best hyperparameters for the LGBMClassifier.\nSet up a parameter grid to search across different values for n_estimators, learning_rate, max_depth, and num_leaves.\nUse 5-fold cross-validation and accuracy as the scoring metric.\nOnce the search is complete, extract the best model with the optimal parameters already fitted to the full training data.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n# optionally filter warnings altogether\n\nlgbm = LGBMClassifier(\n    random_state=0,\n    verbose=-1                # suppress info logs\n)\n\nparam_grid = {\n    'n_estimators': [300, 500, 800],\n    'learning_rate': [0.03, 0.05, 0.1],\n    'max_depth': [-1, 3, 5],\n    'num_leaves': [15, 31, 63],\n    'reg_alpha': [0.0, 0.1, 0.5],\n    'reg_lambda': [0.0, 0.1, 0.5]\n}\n\nsearch = GridSearchCV(\n    estimator=lgbm,\n    param_grid=param_grid,\n    cv=5,                # 5‑fold cross‑validation\n    scoring='accuracy',  # metric to optimize\n    n_jobs=-1            # use all CPU cores\n)\n\nsearch.fit(X, y)\n\nprint(\"Best CV score :\", search.best_score_)\nprint(\"Best params   :\", search.best_params_)\n\nbest_model = search.best_estimator_   # already refit on full data\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:06:01.904205Z","iopub.execute_input":"2025-05-20T16:06:01.905082Z","iopub.status.idle":"2025-05-20T16:22:34.290754Z","shell.execute_reply.started":"2025-05-20T16:06:01.905052Z","shell.execute_reply":"2025-05-20T16:22:34.289861Z"}},"outputs":[{"name":"stdout","text":"Best CV score : 0.8473667691921412\nBest params   : {'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 800, 'num_leaves': 15, 'reg_alpha': 0.5, 'reg_lambda': 0.1}\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"best_model.fit(X_train, y_train)\n\ntrain_acc = best_model.score(X_train, y_train)\nval_acc   = best_model.score(X_val, y_val)\n\nprint(f\"Train accuracy: {train_acc:.4f}\")\nprint(f\"Val accuracy:   {val_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:24:11.175519Z","iopub.execute_input":"2025-05-20T16:24:11.176337Z","iopub.status.idle":"2025-05-20T16:24:11.35676Z","shell.execute_reply.started":"2025-05-20T16:24:11.176308Z","shell.execute_reply":"2025-05-20T16:24:11.355804Z"}},"outputs":[{"name":"stdout","text":"Train accuracy: 0.9073\nVal accuracy:   0.8101\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"Tune XGBoost hyperparameters using GridSearchCV.\nDefine a grid of values for n_estimators, learning_rate, and max_depth.\nUse 5-fold cross-validation and accuracy as the evaluation metric.\nOnce the search completes, extract the best model (XGBoost_best_model) which is already fitted to the full training set.","metadata":{}},{"cell_type":"code","source":"XGBoost = XGBClassifier(random_state=0,use_label_encoder=False, eval_metric='logloss')\nparam_grid = {\n    'n_estimators':[300, 500, 800],\n    'learning_rate': [0.03, 0.05, 0.1],\n    'max_depth': [3, 5, 7],\n    'reg_alpha': [0.0, 0.1, 0.5],\n    'reg_lambda': [0.0, 0.1, 0.5],\n    'subsample': [0.7, 0.8, 1.0]\n}\nsearch = GridSearchCV(\n    estimator=XGBoost,\n    param_grid=param_grid,\n    cv=5,                # 5‑fold cross‑validation\n    scoring='accuracy',  # metric to optimize\n    n_jobs=-1            # use all CPU cores\n)\nsearch.fit(X, y)\n\nprint(\"Best CV score :\", search.best_score_)\nprint(\"Best params   :\", search.best_params_)\n\nXGBoost_best_model = search.best_estimator_   # already refit on full data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:57:28.524737Z","iopub.execute_input":"2025-05-20T16:57:28.52511Z","iopub.status.idle":"2025-05-20T17:04:59.171407Z","shell.execute_reply.started":"2025-05-20T16:57:28.525082Z","shell.execute_reply":"2025-05-20T17:04:59.169949Z"}},"outputs":[{"name":"stdout","text":"Best CV score : 0.8473918774715962\nBest params   : {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 500, 'reg_alpha': 0.5, 'reg_lambda': 0.0, 'subsample': 0.8}\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"XGBoost_best_model.fit(X_train, y_train)\n\ntrain_acc = XGBoost_best_model.score(X_train, y_train)\nval_acc   = XGBoost_best_model.score(X_val, y_val)\n\nprint(f\"Train accuracy: {train_acc:.4f}\")\nprint(f\"Val accuracy:   {val_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T17:06:27.463808Z","iopub.execute_input":"2025-05-20T17:06:27.46414Z","iopub.status.idle":"2025-05-20T17:06:27.71401Z","shell.execute_reply.started":"2025-05-20T17:06:27.464114Z","shell.execute_reply":"2025-05-20T17:06:27.713201Z"}},"outputs":[{"name":"stdout","text":"Train accuracy: 0.9382\nVal accuracy:   0.8212\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"## Ensemble model\nCreate an ensemble model using soft voting.\nRecreate the LightGBM and XGBoost models with the best hyperparameters found earlier.\nCombine them using VotingClassifier with soft voting to average predicted probabilities.\nFit the ensemble on the full training set and evaluate performance using 5-fold cross-validation.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n# Re-create the models with best hyperparameters\nlgbm = LGBMClassifier(\n    learning_rate = 0.03,\n    max_depth= 3,\n    n_estimators=300,\n    num_leaves= 15,\n    reg_alpha= 0.5,\n    reg_lambda= 0.1,\n    random_state=0\n)\n\nxgb = XGBClassifier(\n    learning_rate = 0.5,\n    max_depth = 3,\n    n_estimators = 500,\n    use_label_encoder=False,\n    eval_metric='logloss',\n    random_state=0,\n    reg_alpha = 0.5,\n    reg_lambda = 0.0,\n    subsample = 0.8\n)\n\n# Combine into a soft voting classifier\nensemble = VotingClassifier(\n    estimators=[('lgbm',lgbm),('xgb',xgb)],\n    voting='soft'\n)\n\n#fit on traning data\nensemble.fit(X,y)\ncv_scores = cross_val_score(ensemble, X, y, cv=5, scoring='accuracy')\n\n# Print mean accuracy\nprint(\"Ensemble Cross-Validation Accuracy:\", cv_scores.mean())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T17:09:08.985043Z","iopub.execute_input":"2025-05-20T17:09:08.985378Z","iopub.status.idle":"2025-05-20T17:09:10.660678Z","shell.execute_reply.started":"2025-05-20T17:09:08.985354Z","shell.execute_reply":"2025-05-20T17:09:10.659979Z"}},"outputs":[{"name":"stdout","text":"Ensemble Cross-Validation Accuracy: 0.8338961772644529\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"# Test model\nPrepare the test set using the same steps as the training data:\n• Impute missing values in Age, Embarked, and Fare\n• Create the FamilySize feature\n• Encode categorical variables\n• Align test set columns to match training set features\nMake predictions using the ensemble model and export the results to a CSV file for submission.","metadata":{}},{"cell_type":"code","source":"age_medians = test.groupby(['Pclass','Sex'])['Age'].median()\ndef age_imputer(row):\n    if pd.isna(row['Age']):\n        return age_medians.loc[(row['Pclass'], row['Sex'])]\n    else:\n        return row['Age']\n        \ntest['Age'] = test.apply(age_imputer, axis = 1)\nembarked_mode = test['Embarked'].mode()[0] # grab the single value\ntest['Embarked'] = test['Embarked'].fillna(embarked_mode) # assign back\ntest['Fare'] = test['Fare'].fillna(fare_median)\ntest['FamilySize'] = test['SibSp'] + test['Parch'] + 1\n\ntest['Title'] = test['Name'].str.extract(r',\\s*(\\w+)\\.').iloc[:,0]\ntest['Title'] = test['Title'].replace(['Mlle', 'Ms'], 'Miss')\ntest['Title'] = test['Title'].replace(['Mme', 'Lady', 'Countess'], 'Mrs')\ntest['Title'] = test['Title'].replace(['Dr', 'Rev', 'Col', 'Major', 'Capt', 'Don', 'Sir', 'Jonkheer'], 'Rare')\n\ntest['FarePerPerson'] = test['Fare'] / test['FamilySize']\ntest['FarePerPerson'] = test['FarePerPerson'].replace([np.inf, -np.inf], 0)\ntest['FarePerPerson'] = test['FarePerPerson'].fillna(0)\n\ntest['Deck'] = test['Cabin'].str[0].fillna('U')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T17:10:37.623376Z","iopub.execute_input":"2025-05-20T17:10:37.623694Z","iopub.status.idle":"2025-05-20T17:10:37.655316Z","shell.execute_reply.started":"2025-05-20T17:10:37.623672Z","shell.execute_reply":"2025-05-20T17:10:37.654357Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"num_cols = test[['Age','Fare','FamilySize','FarePerPerson']]\ncat_cols = test[['Sex','Embarked','Pclass','Title', 'Deck']]\n    \ncat_dummies = pd.get_dummies(\n    cat_cols,\n    drop_first=True,\n    dtype='int8'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T17:10:42.503629Z","iopub.execute_input":"2025-05-20T17:10:42.503921Z","iopub.status.idle":"2025-05-20T17:10:42.518827Z","shell.execute_reply.started":"2025-05-20T17:10:42.503901Z","shell.execute_reply":"2025-05-20T17:10:42.51747Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"X_test = pd.concat([num_cols, cat_dummies], axis=1)\n\n# ⚠️  Align columns—make sure test has the same dummy columns as train\nX_test = X_test.reindex(columns=X.columns, fill_value=0)\n\n\npreds = ensemble.predict(X_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T17:10:46.338802Z","iopub.execute_input":"2025-05-20T17:10:46.339126Z","iopub.status.idle":"2025-05-20T17:10:46.360362Z","shell.execute_reply.started":"2025-05-20T17:10:46.339106Z","shell.execute_reply":"2025-05-20T17:10:46.359487Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# 4️⃣  Build the submission file\nsubmission = pd.DataFrame({\n    'PassengerId': test['PassengerId'],\n    'Survived':    preds.astype(int)\n})\nsubmission.to_csv('ensemble2.csv', index=False)\nprint(\"Saved ensemble_submission.csv — ready to upload!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T17:10:56.325738Z","iopub.execute_input":"2025-05-20T17:10:56.326076Z","iopub.status.idle":"2025-05-20T17:10:56.341958Z","shell.execute_reply.started":"2025-05-20T17:10:56.326054Z","shell.execute_reply":"2025-05-20T17:10:56.34082Z"}},"outputs":[{"name":"stdout","text":"Saved ensemble_submission.csv — ready to upload!\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"# 📌 Project Summary: Titanic Survival Prediction\nThis notebook focuses on building a machine learning pipeline to predict passenger survival on the Titanic. The process included thorough data cleaning, exploratory data analysis, feature engineering, and model selection.\n\nKey steps:\n\n* **Data preprocessing:** Handled missing values in Age, Fare, and Embarked using grouped medians and modes based on training data. Created a new FamilySize feature.\n\n* **Feature encoding:** Transformed categorical variables (Sex, Embarked, Pclass) into dummy variables for model compatibility.\n\n* **Model evaluation:** Compared multiple models using 5-fold cross-validation including RandomForest, GradientBoosting, LogisticRegression, XGBoost, and LGBM.\n\n* **Hyperparameter tuning:** Performed GridSearchCV for both XGBoost and LGBM to optimize parameters.\n\n* **Ensembling:** Combined the two best models (tuned XGBoost and LGBM) using a soft voting ensemble to improve generalization.\n\nResults:\n\n* Best cross-validation accuracy from the ensemble: ~0.837\n\n* Best Kaggle submission score: 0.77751, showing steady improvement from the initial baseline.\n\nThis notebook serves as a full pipeline from raw data to submission-ready predictions and marks the completion of one of my first full ML projects.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}